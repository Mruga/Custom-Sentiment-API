{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from slistener.ipynb\n"
     ]
    }
   ],
   "source": [
    "import re,string\n",
    "import math, re, string, requests, json\n",
    "from itertools import product\n",
    "from inspect import getsourcefile\n",
    "from os.path import abspath, join, dirname\n",
    "import nbimporter\n",
    "from slistener import SListener\n",
    "import time, tweepy, sys\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_PUNCTUATION = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "PUNCTUATION_LIST = [\".\", \"!\", \"?\", \",\", \";\", \":\", \"-\", \"'\", \"\\\"\",\n",
    "             \"!!\", \"!!!\", \"??\", \"???\", \"?!?\", \"!?!\", \"?!?!\", \"!?!?\"]\n",
    "\n",
    "NEGATION_WORDS = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    " \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    " \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\"no\",\n",
    " \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    " \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    " \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    " \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    " \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "CONTRASTING_CONJUNCTION = \\\n",
    "{\"but\", \"although\", \"though\", \"even though\", \"even if\", \"however\"}\n",
    "\n",
    "# Sentiment Incremental score for capitalization approach.\n",
    "CAPS_INCR = 1.00\n",
    "\n",
    "# Sentiment score for Punctuation Emphsis symbols\n",
    "QUESTION_INCREASE = 0.1\n",
    "EXCLAMATION_INCREASE = 0.25\n",
    "NEGATION_MODIFIER = -1.00\n",
    "\n",
    "# Score for Degree Modifiers\n",
    "\n",
    "SCORE_INCREASE = 0.5\n",
    "SCORE_DECREASE = -0.5\n",
    "\n",
    "\n",
    "DEGREE_DICTIONARY = \\\n",
    "    {\"100 percent\": SCORE_INCREASE, \"a good deal\": SCORE_INCREASE, \"a great deal\": SCORE_INCREASE,\n",
    "     \"a lot\": SCORE_INCREASE,\n",
    "     \"aboundingly\": SCORE_INCREASE, \"absolutely\": SCORE_INCREASE, \"absurdly\": SCORE_INCREASE,\n",
    "     \"abundantly\": SCORE_INCREASE,\n",
    "     \"admirably\": SCORE_INCREASE, \"alarmingly\": SCORE_INCREASE, \"amazingly\": SCORE_INCREASE,\n",
    "     \"astronomically\": SCORE_INCREASE, \"awfully\": SCORE_INCREASE, \"breathtakingly\": SCORE_INCREASE,\n",
    "     \"clearly\": SCORE_INCREASE, \"completely\": SCORE_INCREASE, \"considerably\": SCORE_INCREASE, \"crazy\": SCORE_INCREASE,\n",
    "     \"damn\": SCORE_INCREASE, \"damned\": SCORE_INCREASE, \"darn\": SCORE_INCREASE, \"darned\": SCORE_INCREASE,\n",
    "     \"decidedly\": SCORE_INCREASE, \"deeply\": SCORE_INCREASE, \"deservedly\": SCORE_INCREASE, \"downright\": SCORE_INCREASE,\n",
    "     \"dreadfully\": SCORE_INCREASE,  # up to here so far\n",
    "     \"effing\": SCORE_INCREASE, \"enormously\": SCORE_INCREASE,\n",
    "     \"entirely\": SCORE_INCREASE, \"especially\": SCORE_INCREASE, \"exceptionally\": SCORE_INCREASE,\n",
    "     \"extremely\": SCORE_INCREASE,\n",
    "     \"fabulously\": SCORE_INCREASE, \"flipping\": SCORE_INCREASE, \"flippin\": SCORE_INCREASE,\n",
    "     \"fricking\": SCORE_INCREASE, \"frickin\": SCORE_INCREASE, \"frigging\": SCORE_INCREASE, \"friggin\": SCORE_INCREASE,\n",
    "     \"fully\": SCORE_INCREASE, \"fucking\": SCORE_INCREASE,\n",
    "     \"greatly\": SCORE_INCREASE, \"hella\": SCORE_INCREASE, \"highly\": SCORE_INCREASE, \"hugely\": SCORE_INCREASE,\n",
    "     \"incredibly\": SCORE_INCREASE,\n",
    "     \"intensely\": SCORE_INCREASE, \"majorly\": SCORE_INCREASE, \"more\": SCORE_INCREASE, \"most\": SCORE_INCREASE,\n",
    "     \"particularly\": SCORE_INCREASE,\n",
    "     \"purely\": SCORE_INCREASE, \"quite\": SCORE_INCREASE, \"really\": SCORE_INCREASE, \"remarkably\": SCORE_INCREASE,\n",
    "     \"so\": SCORE_INCREASE, \"substantially\": SCORE_INCREASE,\n",
    "     \"thoroughly\": SCORE_INCREASE, \"totally\": SCORE_INCREASE, \"tremendously\": SCORE_INCREASE,\n",
    "     \"uber\": SCORE_INCREASE, \"unbelievably\": SCORE_INCREASE, \"unusually\": SCORE_INCREASE, \"utterly\": SCORE_INCREASE,\n",
    "     \"very\": SCORE_INCREASE,\n",
    "     \"almost\": SCORE_DECREASE, \"barely\": SCORE_DECREASE, \"hardly\": SCORE_DECREASE, \"just enough\": SCORE_DECREASE,\n",
    "     \n",
    "     \"less\": SCORE_DECREASE, \"little\": SCORE_DECREASE, \"marginally\": SCORE_DECREASE, \"occasionally\": SCORE_DECREASE,\n",
    "     \"partly\": SCORE_DECREASE,\n",
    "     \"scarcely\": SCORE_DECREASE, \"slightly\": SCORE_DECREASE, \"somewhat\": SCORE_DECREASE,\n",
    "     \"sort of\": SCORE_DECREASE, \"sorta\": SCORE_DECREASE, \"sortof\": SCORE_DECREASE, \"sort-of\": SCORE_DECREASE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Check if there is negation contect in the sentence or not.\n",
    "    Still continuing\n",
    "\"\"\"\n",
    "def negated(input_words, include_nt=True):\n",
    "    \n",
    "    neg_words = []\n",
    "    neg_words.extend(NEGATION_WORDS)\n",
    "      \n",
    "    for word in neg_words:\n",
    "        if word in input_words:\n",
    "            #print(word)\n",
    "            return True\n",
    "    if include_nt:\n",
    "        for word in input_words:\n",
    "            #print(word)\n",
    "            if \"n't\" in word:\n",
    "                return True\n",
    "       \n",
    "    return False\n",
    "\n",
    "def capitalized(words):\n",
    "        \n",
    "        emphasis = False\n",
    "        capitalized_words = 0\n",
    "\n",
    "        for word in words:\n",
    "            if word.isupper():\n",
    "                capitalized_words += 1\n",
    "        cap_differential = len(words) - capitalized_words\n",
    "        if 0 < cap_differential < len(words):\n",
    "            emphasis = True\n",
    "\n",
    "        return emphasis\n",
    "\n",
    "\n",
    "def alter_valence(word, valence, is_cap_diff):\n",
    "   \n",
    "    modifier = 0.0\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in DEGREE_DICTIONARY:\n",
    "        modifier = DEGREE_DICTIONARY[word_lower]\n",
    "        if valence < 0:\n",
    "            modifier = modifier * -1\n",
    "                    \n",
    "        if word.isupper() and is_cap_diff:\n",
    "            if valence > 0:\n",
    "                modifier += CAPS_INCR\n",
    "            else: modifier -= CAPS_INCR\n",
    "    return modifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentimentAnalyzer(object):\n",
    "    \n",
    "    def __init__(self, lexicon_file=\"New_Lexicon.txt\"):\n",
    "        _this_module_file_path_ = abspath(getsourcefile(lambda:0))\n",
    "        lexicon_full_filepath = join(dirname(_this_module_file_path_), lexicon_file)\n",
    "        with open(lexicon_full_filepath) as f:\n",
    "            self.lexicon_full_filepath = f.read()\n",
    "        self.lexicon = self.make_lex_dict()\n",
    "    \n",
    "    def make_lex_dict(self):\n",
    "        \n",
    "        lex_dict = {}\n",
    "        for line in self.lexicon_full_filepath.split('\\n'):\n",
    "            (word, measure) = line.strip().split('\\t')[0:2]\n",
    "            lex_dict[word] = float(measure)\n",
    "            \n",
    "        return lex_dict\n",
    "       \n",
    "\n",
    "    def polarity_scores(self, text):\n",
    "        \n",
    "        sentimenttext = SentimentText(text)\n",
    "        sentiments = []\n",
    "        words_and_emoticons = sentimenttext.words_and_emoticons\n",
    "        \n",
    "        for item in words_and_emoticons:\n",
    "            valence = 0\n",
    "            i = words_and_emoticons.index(item)\n",
    "            if (i < len(words_and_emoticons) - 1 and item.lower() == \"kind\" and\n",
    "                        words_and_emoticons[i + 1].lower() == \"of\") and item.lower() in DEGREE_DICTIONARY:\n",
    "                sentiments.append(valence)\n",
    "            \n",
    "                continue\n",
    "\n",
    "            sentiments = self.sentiment_score(valence, sentimenttext, item, i, sentiments)\n",
    "        sentiments = self._contains_but(words_and_emoticons, sentiments)\n",
    "        \n",
    "        \n",
    "        return(sentiments)\n",
    "    \n",
    "    def _punctuation_emp(self, sum_s, text):\n",
    "       \n",
    "        ep_amplifier = self._exclamation_score(text)\n",
    "        qm_amplifier = self._questionmark_score(text)\n",
    "        punct_emph_amplifier = ep_amplifier+qm_amplifier\n",
    "        return punct_emph_amplifier\n",
    "\n",
    "    def _exclamation_score(self, text):\n",
    "        \n",
    "        ep_count = text.count(\"!\")\n",
    "        if ep_count > 4:\n",
    "            ep_count = 4\n",
    "        \n",
    "        ep_amplifier = ep_count * EXCLAMATION_INCREASE\n",
    "        return ep_amplifier\n",
    "\n",
    "    def _questionmark_score(self, text):\n",
    "        \n",
    "        qm_count = text.count(\"?\")\n",
    "        qm_amplifier = 0\n",
    "        if qm_count > 1:\n",
    "            if qm_count <= 3:\n",
    "                qm_amplifier = qm_count * QUESTION_INCREASE\n",
    "            else:\n",
    "                qm_amplifier = 1.00\n",
    "        return qm_amplifier\n",
    "    \n",
    "    \n",
    "    def _contains_negation(self, valence, words_and_emoticons, start_i, i):\n",
    "        \n",
    "        if start_i == 0:\n",
    "            if negated([words_and_emoticons[i-1]]):\n",
    "                    valence = valence*NEGATION_MODIFIER\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "        if start_i == 1:\n",
    "            if words_and_emoticons[i-2] == \"never\" and\\\n",
    "               (words_and_emoticons[i-1] == \"so\" or\n",
    "                words_and_emoticons[i-1] == \"this\"):\n",
    "                valence = valence*1.5\n",
    "            elif negated([words_and_emoticons[i-(start_i+1)]]):\n",
    "                valence = valence*NEGATION_MODIFIER\n",
    "        if start_i == 2:\n",
    "            if words_and_emoticons[i-3] == \"never\" and \\\n",
    "               (words_and_emoticons[i-2] == \"so\" or words_and_emoticons[i-2] == \"this\") or \\\n",
    "               (words_and_emoticons[i-1] == \"so\" or words_and_emoticons[i-1] == \"this\"):\n",
    "                valence = valence*1.25\n",
    "            elif negated([words_and_emoticons[i-(start_i+1)]]):\n",
    "                valence = valence*NEGATION_MODIFIER\n",
    "\n",
    "        \n",
    "        return valence\n",
    "        \n",
    "        \n",
    "    def sentiment_score(self, valence, sentitext, item, i, sentiments):\n",
    "\n",
    "        is_cap_diff = sentitext.is_cap_diff\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        item_lowercase = item.lower()\n",
    "        if item_lowercase in self.lexicon:\n",
    "            valence = self.lexicon[item_lowercase]\n",
    "            \n",
    "            if item.isupper() and is_cap_diff:\n",
    "                if valence > 0:\n",
    "                    valence += CAPS_INCR\n",
    "                else:\n",
    "                    valence -= CAPS_INCR\n",
    "\n",
    "            \n",
    "            for start_i in range(0,4):\n",
    "                if i > start_i and words_and_emoticons[i-(start_i+1)].lower() not in self.lexicon:\n",
    "                    s = alter_valence(words_and_emoticons[i-(start_i+1)], valence, is_cap_diff)\n",
    "                    valence = valence+s\n",
    "                    valence = self._contains_negation(valence, words_and_emoticons, start_i, i)\n",
    "        \n",
    "        sentiments.append(valence)\n",
    "        return sentiments\n",
    "                     \n",
    "    \n",
    "    \"\"\"\n",
    "    Algorithm is defined as such:\n",
    "    The contextual conjective \"but\" clause works in way to calculate the valence score , algorithm states that the there is 50% \n",
    "    reduction in valence score before \"but\" clause words and 150% increase in the valence score after the \"but\" clause\n",
    "    sentence.\n",
    "\n",
    "    \"\"\" \n",
    "    \n",
    "    def _contains_but(self,words_and_emoticons, sentiments):\n",
    "       \n",
    "        CONTRASTING_CONJUNCTION = \\\n",
    "        {\"but\", \"although\", \"though\", \"even though\", \"even if\", \"however\"}\n",
    "        \n",
    "        for conj_word in CONTRASTING_CONJUNCTION:\n",
    "            if conj_word in words_and_emoticons:\n",
    "                but_index = words_and_emoticons.index(conj_word)\n",
    "                for sentiment in sentiments:\n",
    "                    sentiment_index = sentiments.index(sentiment)\n",
    "                    \n",
    "                    if sentiment_index < but_index:\n",
    "                        sentiments.pop(sentiment_index)\n",
    "                        sentiments.insert(sentiment_index, sentiment * 0.5)\n",
    "                    \n",
    "                    elif sentiment_index > but_index:\n",
    "                        sentiments.pop(sentiment_index)\n",
    "                        sentiments.insert(sentiment_index, sentiment * 1.5)   \n",
    "                        \n",
    "        return sentiments          \n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentimentText(object):\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text.encode('utf-8'))\n",
    "        self.text = text\n",
    "        self.words_and_emoticons = self._words_and_emoticons()\n",
    "        self.is_cap_diff = capitalized(self.words_and_emoticons)\n",
    "        \n",
    "    def _words_and_punc(self):\n",
    "        \n",
    "        no_punc_text = REMOVE_PUNCTUATION.sub('', self.text)\n",
    "        words_only = no_punc_text.split()\n",
    "        words_only = set( w for w in words_only if len(w) > 1 )\n",
    "        punc_before = {''.join(p): p[1] for p in product(PUNCTUATION_LIST, words_only)}\n",
    "        punc_after = {''.join(p): p[0] for p in product(words_only, PUNCTUATION_LIST)}\n",
    "        words_punc_dict = punc_before\n",
    "        words_punc_dict.update(punc_after)\n",
    "        return words_punc_dict\n",
    "\n",
    "    def _words_and_emoticons(self):\n",
    "        \n",
    "        wes = self.text.split()\n",
    "        words_punc_dict = self._words_and_punc()       \n",
    "        wes = [we for we in wes if len(we) > 1]\n",
    "        for i, we in enumerate(wes):\n",
    "            if we in words_punc_dict:\n",
    "                wes[i] = words_punc_dict[we]\n",
    "        return wes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    analyzer = SentimentAnalyzer()\n",
    "    Idioms_data = pd.read_csv(\"Idioms_File.csv\")\n",
    "    text_sentence = Idioms_data['definition']\n",
    "    def make_list():\n",
    "        result = []\n",
    "        for sentence in text_sentence:\n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            total_score = format(float(sum(vs)),'.2f')\n",
    "            result.append(total_score)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = make_list()\n",
    "Idioms_data['Sentiment_Score'] = temp\n",
    "Idioms_data.to_csv(\"Idioms_Sentiment_Score.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
